{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2adb442-842f-4afd-8c6a-85014a19391f",
   "metadata": {},
   "source": [
    "**Q1. You are working on a mach#ne learning project where you have a dataset containing numerical and\n",
    "categorical features. You have identified that some of the features are highly correlated and there are\n",
    "missing values in some of the columns. You want to build a pipeline that automates the feature\n",
    "engineering process and handles the missing values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a8d102-da22-4032-ab42-90b2df33ba15",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "Certainly! To address the feature engineering and missing value handling in your machine learning project pipeline, you can follow these steps using Python and scikit-learn:\n",
    "\n",
    "### Steps to Build the Pipeline:\n",
    "\n",
    "1. **Import Libraries:**\n",
    "   - Import necessary libraries and modules from scikit-learn for building the pipeline, handling missing values, and performing feature engineering.\n",
    "\n",
    "2. **Load the Dataset:**\n",
    "   - Load your dataset containing numerical and categorical features. Ensure you have identified which columns have missing values and which features are highly correlated.\n",
    "\n",
    "3. **Preprocessing Steps:**\n",
    "   - Define preprocessing steps such as handling missing values, encoding categorical variables, and scaling numerical features.\n",
    "\n",
    "4. **Feature Engineering:**\n",
    "   - Optionally, perform feature engineering steps such as creating new features, transforming existing features, or selecting important features.\n",
    "\n",
    "5. **Pipeline Construction:**\n",
    "   - Build a pipeline using `Pipeline` from scikit-learn to sequentially apply the defined preprocessing and feature engineering steps.\n",
    "\n",
    "6. **Training the Pipeline:**\n",
    "   - Train your pipeline on the dataset, which includes fitting any necessary parameters or transformations.\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "- **Step 1:** Import necessary modules and classes.\n",
    "- **Step 2:** Load the dataset (`iris` dataset in this example).\n",
    "- **Step 3:** Define preprocessing steps:\n",
    "  - `SimpleImputer` is used to handle missing values (`strategy='mean'` for numerical, `strategy='most_frequent'` for categorical).\n",
    "  - `StandardScaler` scales numerical features to have zero mean and unit variance.\n",
    "  - `OneHotEncoder` encodes categorical features into binary vectors.\n",
    "- **Step 4:** `ColumnTransformer` is used to apply different preprocessing steps to numerical and categorical features.\n",
    "- **Step 5:** Build a `Pipeline` where `preprocessor` handles preprocessing and `classifier` (in this case `RandomForestClassifier`) is used as the model.\n",
    "- **Step 6:** Train the pipeline on training data (`X_train`, `y_train`).\n",
    "- **Step 7:** Evaluate the pipeline's accuracy on the test set (`X_test`, `y_test`).\n",
    "\n",
    "### Interpretation and Improvements:\n",
    "\n",
    "- **Interpretation of Results:** The accuracy score gives an estimate of how well the pipeline performs in predicting the target variable on unseen data.\n",
    "  \n",
    "- **Possible Improvements:** \n",
    "  - **Advanced Imputation:** Explore more sophisticated imputation techniques.\n",
    "  - **Feature Selection:** Use techniques like feature selection to remove less informative features.\n",
    "  - **Hyperparameter Tuning:** Optimize hyperparameters of both the preprocessing steps and the classifier to improve performance.\n",
    "\n",
    "This pipeline automates the process of handling missing values and performing feature engineering, providing a structured approach to preprocessing your dataset for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ff5b5d-c364-4031-9ea9-75e5baf86579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "# Assuming you have a dataset `X` and `y`, where `X` contains features and `y` contains the target variable\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Step 3: Define preprocessing steps\n",
    "# Handle missing values and scale numerical features\n",
    "numeric_features = [0, 1, 2, 3]  # Example: indices of numerical columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values with mean imputation\n",
    "    ('scaler', StandardScaler())  # Scale numerical features\n",
    "])\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_features = []  # Example: indices of categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values with most frequent imputation\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Step 5: Build the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())  # Example classifier, replace with your choice\n",
    "])\n",
    "\n",
    "# Step 6: Train the pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate accuracy\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7840d-274a-49b8-8d7c-4ab8eeea24fc",
   "metadata": {},
   "source": [
    "**Design a pipeline that includes the following steps\":**\n",
    "\n",
    "Use an automated feature selection method to identify the important features in the dataset\n",
    "\n",
    "Create a numerical pipeline that includes the following steps\"\n",
    "\n",
    "Impute the missing values in the numerical columns using the mean of the column values\n",
    "\n",
    "Scale the numerical columns using standardisatiom\n",
    "\n",
    "Create a categorical pipeline that includes the following steps\"\n",
    "\n",
    "Impute the missing values in the categorical columns using the most frequent value of the column\n",
    "\n",
    "One-hot encode the categorical columns\n",
    "\n",
    "Combine the numerical and categorical pipelines using a ColumnTransformer\n",
    "\n",
    "Use a Random Forest Classifier to build the final model\n",
    "\n",
    "Evaluate the accuracy of the model on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb448ca0-10fc-4083-b943-4fbfe7f03446",
   "metadata": {},
   "source": [
    "Note! Your solution should include code snippets for each step of the pipeline, and a brief explanation of each step. You should also provide an interpretation of the results and suggest possible improvements for the pipeline.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404de16-1268-4ee6-86b5-a82b5c53feab",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "\n",
    "To design a pipeline as per your requirements, we'll use Python and scikit-learn to automate feature selection, handle missing values, perform standardization, one-hot encoding, and finally build and evaluate a Random Forest Classifier. Here's how you can construct this pipeline step-by-step:\n",
    "\n",
    "### Steps to Design the Pipeline:\n",
    "\n",
    "1. **Import Libraries:**\n",
    "   - Import necessary libraries and modules from scikit-learn for feature selection, preprocessing, modeling, and evaluation.\n",
    "\n",
    "2. **Load the Dataset:**\n",
    "   - Load your dataset containing both numerical and categorical features.\n",
    "\n",
    "3. **Automated Feature Selection:**\n",
    "   - Choose an automated feature selection method to identify important features. Here, we'll use `SelectFromModel` with a RandomForestClassifier as an example.\n",
    "\n",
    "4. **Numerical Pipeline:**\n",
    "   - Create a pipeline for numerical features:\n",
    "     - Impute missing values using mean imputation (`SimpleImputer`).\n",
    "     - Scale numerical columns using standardization (`StandardScaler`).\n",
    "\n",
    "5. **Categorical Pipeline:**\n",
    "   - Create a pipeline for categorical features:\n",
    "     - Impute missing values using the most frequent value (`SimpleImputer`).\n",
    "     - One-hot encode categorical columns (`OneHotEncoder`).\n",
    "\n",
    "6. **Combine Pipelines:**\n",
    "   - Use `ColumnTransformer` to combine the numerical and categorical pipelines.\n",
    "\n",
    "7. **Build Final Pipeline:**\n",
    "   - Build a complete pipeline:\n",
    "     - Feature selection.\n",
    "     - Combined numerical and categorical preprocessing.\n",
    "     - Random Forest Classifier as the final model.\n",
    "\n",
    "8. **Train and Evaluate:**\n",
    "   - Train the pipeline on the training dataset.\n",
    "   - Evaluate the accuracy of the model on the test dataset.\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "- **Step 1:** Import necessary modules and classes from scikit-learn.\n",
    "- **Steps 2-3:** Load dataset (example using Iris dataset) and choose feature selection method (`SelectFromModel` with `RandomForestClassifier`).\n",
    "- **Steps 4-5:** Define pipelines for numerical and categorical features:\n",
    "  - `SimpleImputer` handles missing values (`mean` for numerical, `most_frequent` for categorical).\n",
    "  - `StandardScaler` scales numerical features.\n",
    "  - `OneHotEncoder` encodes categorical features.\n",
    "- **Step 6:** Use `ColumnTransformer` to apply preprocessing steps to respective feature types.\n",
    "- **Step 7:** Construct a pipeline:\n",
    "  - `SelectFromModel` selects important features.\n",
    "  - `ColumnTransformer` combines preprocessing steps for numerical and categorical features.\n",
    "  - `RandomForestClassifier` as the final model.\n",
    "- **Step 8:** Train-test split (`train_test_split`) and evaluate the pipeline's accuracy on the test set (`accuracy_score`).\n",
    "\n",
    "### Interpretation and Improvements:\n",
    "\n",
    "- **Interpretation of Results:** The accuracy score on the test set indicates how well the pipeline, including feature selection, preprocessing, and modeling, performs in predicting the target variable.\n",
    "  \n",
    "- **Possible Improvements:** \n",
    "  - **Hyperparameter Tuning:** Optimize hyperparameters of `RandomForestClassifier` and other components.\n",
    "  - **Feature Engineering:** Explore additional feature engineering techniques.\n",
    "  - **Cross-validation:** Use cross-validation for more robust evaluation.\n",
    "\n",
    "This pipeline encapsulates the process of automated feature selection, handling missing values, and combining numerical and categorical feature preprocessing, leading to a streamlined approach for building and evaluating a machine learning model. Adjustments can be made based on specific dataset characteristics and modeling goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8d5e6ab-ec1b-42a6-8256-0aeb844fadd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert X_train and X_test to DataFrames if they are arrays\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "# Numeric features preprocessing\n",
    "numeric_features = list(X_train.select_dtypes(include=['int64', 'float64']).columns)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical features preprocessing\n",
    "categorical_features = list(X_train.select_dtypes(include=['object']).columns)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the pipeline with the classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c5003f-a6a5-4c1f-9de5-f97939163bc1",
   "metadata": {},
   "source": [
    "**Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b683b9-3a84-4e7b-85b3-2779678b4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8bd48d3-7602-46a6-a911-f689228976aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d046024-3140-4e73-9609-3a0659ee7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for Random Forest Classifier\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline for Logistic Regression Classifier\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ba17d16-26e8-4572-a972-c213cf530921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Voting Classifier combining both pipelines\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', rf_pipeline),\n",
    "    ('lr', lr_pipeline)\n",
    "], voting='soft')  # 'soft' voting uses predicted probabilities for decision\n",
    "\n",
    "# Create a final pipeline with the Voting Classifier\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('voting', voting_clf)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b987b3df-7362-4260-aeff-2e17a55cdb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline on the training data\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36732cd1-dfc1-4575-9a32-d17bc8155c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
